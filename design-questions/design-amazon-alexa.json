{
    "uid": "design-amazon-alexa",
    "name": "Design Amazon Alexa",
    "acl": {
        "isFree": false,
        "isFreeForStudents": false,
        "productRequired": [
            "mlexpert"
        ],
        "isAvailable": true
    },
    "releaseDate": "2021-07-18T12:00:00-04:00",
    "isReleased": true,
    "video": {
        "vimeoId": "576566532",
        "duration": 0,
        "annotations": [],
        "instructor": "",
        "style": null
    },
    "prompt": "<p>\n  Alexa is Amazon's custom-built voice assistant.\n</p>\n",
    "walkthrough": [
        {
            "title": "Gathering System Requirements",
            "content": "<p>\n    As with any ML design interview question, the first thing that we want to\n    do is to gather requirements; we need to figure out what we need to build\n    and what we don't need to build.\n</p>\n<p>\n    We're designing Amazon Alexa, a voice assistant. The voice assistant will\n    delegate tasks to pre-created \"skills\" (weather, Uber, Spotify, etc).\n</p>\n<p>To accomplish this we'll need to design 5 components:</p>\n<ul>\n    <li>Keyword Spotter</li>\n    <li>Automatic Speech Recognizer</li>\n    <li>Natural Language Understanding Engine</li>\n    <li>Dialog Manager</li>\n    <li>Text-to-Speech Engine</li>\n</ul>\n\n<p>We'll need to rely on systems which we won't touch on or design:</p>\n<ul>\n    <li>Amazon's Echo Dot, a device used to interface with customers which\n    includes a speaker and an array of microphones.</li>\n    <li>AWS which is a cloud infrastructure to host our microservies.</li>\n</ul>"
        },
        {
            "title": "Keyword Spotter (KWS)",
            "content": "<p>\n    The KWS will be hosted on the Echo Dot which will listen for the wake word.\n    The wake word will signal the device to begin recording the customer's\n    request. Once the customer stops speaking, the MP3 audio file will be\n    streamed to the cloud. In our case, this will be 48 kHz sample rate at\n    a 16-bit depth.\n</p>\n<p>Since we're hosting the KWS on-device, we'll have a couple of constraints:</p>\n<ul>\n    <li>The model needs to be small enough to fit in the limited memory.</li>\n    <li>The model also has to have a limited number of parameters such that the processor can\n    run predictions with minimal latency</li>\n</ul>\n<p>\n    Keeping these constraints in mind, we'll also need to ensure the model will\n    provide a low enough False Rejection Rate (false negative in detecting wake word)\n    and False Acceptance Rate (false positive in detecting wake word) so that we\n    don't frustrate users or compromise user privacy.\n</p>\n<p>\n    Architecturally, the KWS will operate on a 1 second sliding window of audio.\n    This sliding window comes from the fact that the 90th percentile of users\n    require 900ms to say the wake word (WW).\n\n    To get features from the sliding window audio signal, we'll be creating\n    a spectrogram. From the 1 second of audio, we'll extract 25 millisecond (ms)\n    subsections with an overlap of 10 ms. The short-time discrete Fourier\n    transform will be used on each subsection to create a periodogram. A series\n    of overlapping Mel-spaced triangle filters will be used to extract numerical\n    features. In total, 26 features will represent a single 25 ms subsection.\n    Finally, we take the log of all the features across all of the subsections\n    to create a log Mel filter bank energy spectrogram. This spectrogram will\n    be used as the features for our model.\n\n    We'll use a convolutional neural network with to predict whether or not the\n    features contain the wake word. However, since not all wake words will align\n    with the 1 second window, we'll need to have three possibilities output from\n    the convolutional neural network. One, the wake word ending center of the\n    window. Two, the wake word ending after the center of the window. And three,\n    the wake word starting at the end of the window.\n\n    Finally, to account for noise, we'll need to perform mean subtraction on the\n    spectrogram.\n</p>"
        },
        {
            "title": "Automatic Speech Recognizer (ASR)",
            "content": "<p>\n    The ASR will be hosted in the cloud. It will take in raw audio, featurize it\n    with a spectrogram, and output text representing the spoken language in the\n    raw audio.\n</p>\n<p>\n    Architecturally, the ASR will start with a 1-dimensional convolutional neural\n    network. This will assist in representing n-grams from the phonemes.\n    That result will feed into a bidirectional recurrent neural network which will\n    encode a sequence provided by the convolutional neural network.\n\n    Finally, the model will have a fully connected layer with a softmax outputting\n    characters and punctuation. The connectionist temporal classification (CTC) loss\n    function will be used to train the network. The CTC loss function assists in\n    aligning the per character prediction corresponding to the provided\n    spectrogram. Without it, the labelling of the dataset per character with\n    each timestep in the spectrogram would be expensive and would also likely\n    not generalize to different speaker paces. CTC uses beam search to efficiently\n    evaluate every possible valid alignment of characters associated with a\n    spectrogram.\n</p>"
        },
        {
            "title": "Natural Language Understanding Engine (NLU)",
            "content": "<p>\n    The NLU engine will be hosted in the cloud. It will take in text output\n    from the ASR and transform it into a structured format expected by the\n    Dialog Manager. This format will require the NLU engine to extract a domain,\n    an intent, and a number of slots.\n\n    The domains consist of RideServices, Music, Groceries, etc. Intents are\n    associated with an individual domain. For instance, the Music domain could\n    have the intent PlaySong if a user wants to play a song on Spotify or\n    GetArtistFromSongTitle if a user wants to know the artist of a given song.\n    Finally, slots provided the information required with the intent. For the\n    PlaySong intent, a slot would require the song title and perhaps an artist\n    if it can't be implied.\n\n    Each intent will be associated with a core utterance. For instance, RideService\n    intent may use 'Call me an Uber to X.' as a core utterance. Slots will also\n    be specified for an intent or 'skill'. An example of a slot for the RideService\n    intent could be a destination such as work or airport. These slots can be\n    refined after the skill is created based on data obtained after the launch.\n</p>\n<p>\n    Architecturally, the model we create will have to be a multi-tasking model.\n    Each domain will be a task for the model to learn since each domain will have\n    separate features and labels.\n\n    The first layer of the model will be an embedding layer. This will allow the\n    text produced by the ASR to be represented in a euclidean space. This means\n    words will be further or closer to one another based on their usage similarities.\n    A pre-trained embedding layer can be used here such as fastText.\n\n    The embedding layer will feed into a shared bidirectional LSTM to encode the\n    sequence of words. Then, two more bidirectional LSTM layers will be used.\n    One for intent classification and the other for slot tagging. These two\n    layers will be optimized independently for each domain while the original\n    shared bidirectional LSTM will be trained across domains. This shared layer\n    is what creates a multi-task model.\n\n    The intent classification layer will use a softmax to predict which intent\n    the user is specifying. The slot tagging layer will label each word in the\n    input based on the inside-outside-beginning (IOB) format. Both the intent\n    classification layer and the slot tagging layer use a loss function which\n    incorporates the performance of each other as well as itself. This means the\n    objective is in the category of 'joint learning'.\n\n    In the case of a new intent being added, we'll need to be able to use data\n    from other intents because there will be limited training examples\n    for newly created skills. We can use a technique called transfer learning.\n    This can be done by pre-training the network with popular intents. The\n    pre-trained network will then be equipped with untrained softmaxes and\n    then further trained with the limited examples available for the new intent\n    resulting in a network that leverages popular intents to support new intents.\n</p>"
        },
        {
            "title": "Dialog Manager (DM)",
            "content": "<p>\n    The DM will control the flow of the conversation of the voice assistant in\n    accordance with the requirements of a specific intent. As well, the DM will\n    be responsible for delegating requests to dedicated services to fulfill users'\n    intents. For instance, if a user says 'Call me an Uber' the DM will specify\n    that more information is necessary and reply with 'Where would you like to go?'\n    After gathering the required information for the RideService intent, the DM\n    will call the micro-service dedicated to maintaining the RideService logic\n    to complete the user's request and arrange an Uber.\n\n    The DM will have access to a set of schemas which will list required and\n    optional slots for a given intent. For example if a user specifies 'Play a\n    song', the DM will be able to look up the PlayMusic intent schema and see\n    that at least a genre is required. As well, the schema will specify text\n    which will be sent to the text-to-speech engine such that the genre field\n    can be populated e.g. 'What genre of music would you like to play?'\n\n    As well, the DM will provide validation or inform users of problems reported\n    by intent services e.g. 'Your Uber is on the way.' The DM will also be\n    responsible for maintaining humanistic qualities. For instance, if a\n    micro-service is taking too long to respond, the DM can fill the silence with\n    'Hmm, let me think about that'.\n</p>"
        },
        {
            "title": "Text-to-speech Engine (TTS)",
            "content": "<p>\n    The TTS will be responsible for taking the output of the DM and transforming\n    it into an audio signal to be sent to the Echo Dot and played back for the\n    user to hear. This will allow users to get confirmations of orders, have\n    questions answered, or to hear a weather forecast.\n</p>\n<p>\n    Architecturally, the components of TTS include:\n    <ul>\n        <li>Spectrogram Prediction</li>\n        <li>Neural Vocoder</li>\n    </ul>\n\n    The spectrogram prediction model will be responsible for producing a\n    spectrogram from the text generated by the DM. The neural vocoder will take\n    the produced spectrogram and transform it into an audio signal.\n</p>\n<p>\n    The spectrogram prediction model will take the form of an encoder-decoder.\n    The encoder will take in a series of words and encode it into a fixed\n    representation. This representation will then be sent to the decoder to be\n    decoded into a spectrogram. Since the encoder produces a fixed length output\n    and the encoder input can be much longer, an attention mechanism is required\n    to ensure that the the overall meaning doesn't get lost while condensing a\n    long sequence to a fixed dimension.\n\n    The encoder will begin with an embedding layer such that each word gets\n    transformed into an embedding. Next, a convolutional layer is used to allow\n    for the representation of n-grams. Lastly, the encoder is layered with a\n    bidirectional LSTM so that the model can encode the given sequence.\n\n    The attention layer is used between the encoder and decoder to assign\n    probabilities of each input step being associated with a particular output\n    step of the encoder. This improves the performance of the model when provided\n    longer sequences than the fixed length representation used by the encoder.\n\n    The decoder is first equipped with a unidirectional LSTM which feeds the\n    previous timestep's output back to the attention layer. This creates a\n    location-based attention mechanism which encourages the model to avoid\n    common failure modes such as ignoring portions of a sequence or repeating\n    portions of a sequence. The output of the unidirectional LSTM is fed into a\n    single layer fully-connected neural network which will output the values of\n    each frequency on the spectrogram at each timestep. As well, a pre-net\n    (2-layer feed-forward neural network) is used alongside the unidirectional\n    LSTM to provide the LSTM with the previous output of the decoder. A post-net\n    (convolutional neural network) has shown to improve the reconstruction of the\n    spectrogram by the decoder. Lastly, a single layer feed-forward neural network\n    followed by a sigmoid to predict the end of the generated spectrogram.\n</p>\n<p>\n    The neural vocoder is responsible for taking the generated spectrogram and\n    creating an audio signal. A target audio signal along with the spectrogram\n    associated with the audio are used to train the vocoder. During inference,\n    the goal of the vocoder is the be able to synthesize an audio signal given\n    just a spectrogram (here generated by the previous model).\n\n    Architecturally, the spectrogram is first fed into a transpose convolution,\n    sometimes called a deconvolution because it does the same thing as a\n    convolution but in reverse. This effectively upsamples the spectrogram\n    allowing us to match the dimensions of the target audio signal input. The\n    target audio signal is sent through a 1 by 1 convolutional layer to add\n    channels to the signal. It doesn't change the signal dimensions apart from\n    the depth. These added channels are sometimes called residual channels.\n    The convolved audio signal is then sent through a dilated convolutional\n    layer which does the same thing as a normal convolution except that there\n    are gaps in the values considered for convolution. Dilated convolutions\n    allow for a greater receptive field while maintaining the same number of\n    parameters. The output of the dilated convolution is added to the output of\n    the transpose convolution. This sum is then fed separately into a tanh and\n    a sigmoid. This creates a 'gating' mechanism similar to what is seen in an\n    LSTM. The output of the gate is used twice. Once for being fed into a\n    subsequent layer, and the other for being added to the original target audio\n    signal to then be used as the second input to a subsequent layer. Here the\n    subsequent layer is a residual layer which consists of the the same components\n    discussed in this paragraph.\n\n    A series of 30 residual layer outputs are summed up, sent through a series\n    of ReLUs and 1 by 1 convolutions to be finally sent through a mixture of\n    logistic distributions (MoL). The MoL allows us to predict the mean and\n    standard deviation of a logistic distribution from which we select the most\n    probable value distribution. A mixture of logistic distributions is used\n    (here 10) for the cases in which the true distribution does not exactly\n    follow a logistic. The mixture can roughly approximate many more distributions.\n    The most probable value from the mixture of these distributions is selected\n    as the value for the synthesized audio at that timestep. Since each step of\n    the audio is represented with 16 bits, we approximate the value produced by\n    the MoL to the nearest 16-bit number. Since our audio sample rate is 48 kHz,\n    we'll use this TTS engine to synthesize 48000 values per second of audio.\n</p>"
        },
        {
            "title": "KWS Diagram",
            "content": "<img\n    width=\"100%\"\n    src=\"https://assets.algoexpert.io/mle/alexa2.svg\"\n    alt=\"KWS Diagram\"\n    style=\"background-color:white;\"\n/>"
        },
        {
            "title": "ASR Diagram",
            "content": "<img\n    width=\"100%\"\n    src=\"https://assets.algoexpert.io/mle/alexa1.svg\"\n    alt=\"ASR Diagram\"\n    style=\"background-color:white;\"\n/>"
        },
        {
            "title": "TTS Neural Vocoder Diagram",
            "content": "<img\n    width=\"100%\"\n    src=\"https://assets.algoexpert.io/mle/alexa3.svg\"\n    alt=\"TTS Neural Vocoder Diagram\"\n    style=\"background-color:white;\"\n/>"
        },
        {
            "title": "TTS Spectrogram Prediction Diagram",
            "content": "<img\n    width=\"100%\"\n    src=\"https://assets.algoexpert.io/mle/alexa4.svg\"\n    alt=\"TTS Spectrogram Prediction Diagram\"\n    style=\"background-color:white;\"\n/>"
        }
    ],
    "hints": [
        {
            "question": "How will users interact with Alexa?",
            "answer": "Users will interact with Alexa through an Amazon Echo Dot, which is a small device that can be placed on any surface, like a table or a countertop."
        },
        {
            "question": "Will users initilize an interaction with a wake word? If so, do I need to implement that?",
            "answer": "Yes, you'll need to implement wake-word detection. The wake word will be \"Alexa\", though the exact word shouldn't matter much for this exercise."
        },
        {
            "question": "Can I assume that only a single user will be speaking at once in a relatively quiet room?",
            "answer": "Yes."
        },
        {
            "question": "Should Alexa be able to handle multiple languages?",
            "answer": "No. A single language is okay."
        },
        {
            "question": "Can I assume that the user is a native english speaker?",
            "answer": "Yes, that's fine for this interview."
        },
        {
            "question": "What capabilities do we need to support as a voice assistant?",
            "answer": "For now, let's assume that we have about a dozen \"skills\" that can be called on (e.g., asking for the weather forecast,ordering food, fetching a ride, playing a song, etc.)."
        },
        {
            "question": "Do any of the skills require speaker recognition? For example, should Alexa behave differently depending on who's asking a question?",
            "answer": "For now, let's assume that the skills don't require speaker recognition."
        }
    ]
}