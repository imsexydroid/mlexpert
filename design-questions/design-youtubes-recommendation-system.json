{
    "uid": "design-youtubes-recommendation-system",
    "name": "Design YouTube's Recommendation System",
    "acl": {
        "isFree": false,
        "isFreeForStudents": false,
        "productRequired": [
            "mlexpert"
        ],
        "isAvailable": true
    },
    "releaseDate": "2021-08-22T12:00:00-04:00",
    "isReleased": true,
    "video": {
        "vimeoId": "590524965",
        "duration": 0,
        "annotations": [],
        "instructor": "",
        "style": null
    },
    "prompt": "<p>\n  Design a system to recommend YouTube videos to users.\n</p>\n",
    "walkthrough": [
        {
            "title": "Gathering System Requirements",
            "content": "<p>\n    As with any ML design interview question, the first thing that we want to\n    do is to gather requirements; we need to figure out what we need to build\n    and what we don't need to build.\n</p>\n<p>\n    We're designing YouTube's recommendation system.\n</p>\n<p>To accomplish this we'll need to design 3 components:</p>\n<ul>\n    <li>Personalized Recommendation Model</li>\n    <li>Distributed Training, Tuning, and Evaluation</li>\n    <li>Model Hosting</li>\n</ul>\n\n<p>We'll need to rely on systems which we won't touch on or design:</p>\n<ul>\n    <li>Cloud Compute and Storage infrastructure (AWS or GCP)</li>\n    <li>Retraining or Online Learning</li>\n    <li>User Interface</li>\n    <li>Model Development</li>\n    <li>Online experimentation such as A/B Testing</li>\n    <li>The transformation of raw data into useable features and labels</li>\n</ul>"
        },
        {
            "title": "Personalized Recommendation Model",
            "content": "<p>\n  The problem we're trying to solve is video recommendations. However, there's\n  typically a surrogate problem that we optimize ML models for. Our surrogate\n  problem here will be trying to predict the next video that a user will\n  watch. Framing the problem this way allows us to use features which include\n  historical interactions a user has had with videos. Labels will then be assigned\n  based on subsequent videos that a user has watched. This way, a model will\n  predict the next video that a user will watch and that will be presented to\n  the user as a recommendation. To do this, we'll need the following:\n</p>\n<ul>\n    <li>Candidate Generators</li>\n    <li>A Ranking Model</li>\n    <li>Bias Mitigators</li>\n    <li>Cold-start Mitigators</li>\n</ul>\n<p>\n    The candidate generator is responsible for narrowing millions of possible\n    video recommendations down to hundreds of recommendation candidates. It aims\n    to provide a relevant candidate subset with a high degree of precision (as opposed\n    to recall). This allows us to later use a model to score these candidates\n    with a more detailed representation of users and videos without having to consider\n    millions of videos. This two-stage approach allows us to provide inferences with\n    reasonable latency also allows us to use more than a single candidate generator.\n\n    The features we use for the candidate generator should include at least the\n    video watch history. Here, we consider a video as watched if the user watched\n    an entire video to completion. To represent the historical watches, we'll\n    use a continuous bag of words (CBOW) embedding. For the input to the CBOW each\n    video will be represented as a one-hot encoding of the million most popular\n    videos and the embedding layer creates a dense representation of the video.\n    The CBOW will consider the previous 50 video watches as input. Since each of these\n    one-hot encoded videos will produce a dense vector after the embedding layer,\n    we'll average the 50 embeddings together to get a single, fixed-length representation\n    of a user's watch history. If a watched video is not present in the list of the\n    one million most popular videos, that video will be assigned to a zero vector\n    instead of being one-hot encoded.\n\n    In addition to historical watches, we can include historical search queries\n    the user made. We can use a similar approach to historical watches with another\n    CBOW embedding. Each of the previous 50 search queries will be tokenized into\n    unigrams and bigrams. Bigrams will be selected by how frequently unigrams are\n    adjacent to each other. These tokens will be one-hot encoded among the most frequent\n    one million tokens. If a token a user searched is not in the set of the one\n    million most frequent tokens, a vector of zeros will be used to represent that\n    query. Similarly, the 50 search query embeddings will be averaged together\n    to obtain one fixed-length embedding of the user search history.\n\n    The label for the model will be the subsequent video watched by the user.\n    Here, that means the label will be the one-hot encoded 51st video the user\n    watched. Since the output of the model is also a one-hot encoded video, it\n    means that we have a multiclass prediction across one million classes. This\n    is called an extreme multiclass classification problem. Calculating\n    these million class probabilities for hundreds of billions of training examples\n    is intractable so we'll have to use candidate sampling. This just means we'll\n    sample a few thousand of the incorrect class outputs as well as the correct\n    class output and only consider the gradient calculation among the sampled\n    classes (instead of all one million classes). Classes will be sampled in\n    proportion to the frequency that the label appears in the training set. As well,\n    we'll need to use importance sampling to account for only updating samples of\n    the class parameters and not all of them. This just means we'll scale the impact\n    each class has on a parameter update in proportion to how much our sampling\n    technique differs from updating the class parameters without sampling. This\n    allows us to speed up training significantly without heavily influencing or\n    some cases, improving the model performance.\n\n    Architecturally, a maximum of 50 one-hot encoded vectors for each the video\n    and search histories will be fed into separate CBOW embedding layers to produce\n    a 256 dimension embedding. This will feed into fully-connected layer of\n    2048 units with the ReLU activation function. Each successive layer will halve\n    the number of units until a 256 unit layer remains. This layer represents an\n    embedding for the user. A 256 unit embedding is appended to the network which feeds\n    into a million unit softmax layer which predicts the next video a user will\n    watch. The 256 unit embedding directly preceding the softmax layer represents\n    an unnormalized distribution of the one million video in a 256 dimensional space.\n    This means that the softmax layer just decodes the video embedding into a\n    normalized probability distribution across all one million videos.\n\n    Once trained, an unseen example is provided to the model. The model will produce\n    a 256 dimension user embedding representing a single user. The dot product of\n    the user embedding and any of the million video embeddings represents the\n    similarity between the user embedding and the video embedding. Theoretically,\n    we can find the dot product for all one million videos and the user embedding.\n    The videos producing the highest resulting dot products can be used as recommendation\n    candidates. We'll talk more about how to do this practically in the Model Hosting\n    section to avoid one million dot product calculations during inference.\n\n    The offline evaluation metric we'll use for the candidate generator is the mean\n    average precision for the top k candidates (MAP@k) produced by the test set.\n\n    We can improve the MAP by rejecting candidates based on explicit feedback such\n    as dislikes or implicit feedback like partial watches.\n</p>\n<p>\n    The ranking model is used to score each of the recommendation candidates returned\n    by the candidate generator. Since the candidate generator only provides a few\n    hundred videos to the ranking model, we can use a richer feature set than the\n    candidate generator and still maintain a reasonable amount of latency and\n    cost per recommendation. Finally, the ranking model is useful in the case that\n    we want to use more than one candidate generator.\n\n    Some of the ranking features will include the same features used by the candidate\n    generator such as a user's previously watched videos. We can still average\n    the CBOW embeddings for the videos watched (and searches) by the user to summarize\n    the user's preference. We can also include specific features of how long it's\n    been since a user watched a video from a particular channel, how long it's been\n    since a user watched a video about a certain topic, how many videos have been\n    impressed on the user, and finally we can even provide the candidate generators\n    score of the item (in this case the distance from the user/video embedding dot\n    product to the candidate video). For these continuous features, scaling significantly\n    impacts model performance. Using normalization ensures each feature lies between\n    0 and 1. As well, we can provide transformations of the continuous features to\n    the model including the squared feature and the square root of the feature.\n    It's important to note that features obtained from videos (last video watched,\n    last video shown to the user) all use the same embedding to learn a generalized\n    representation of videos. The same is true for search terms. Since we're using\n    far more features in the ranking model than the candidate generator, we should\n    use a fewer number of embedding units such as 32 instead of 256 to speed up the\n    latency of inferences. In addition, we'll also want to incorporate the\n    impressed videos that a user was shown for that particular training example.\n    This impressed video is also sent through the CBOW layer but instead of averaging\n    the resulting embedding along with the video watch history, it will be concatenated\n    alongside the other features we provide to the ranking model. The label will\n    inform the network whether the user clicked on that impression.\n\n    Labels will simply be binary based on whether or not a user clicked on the\n    impressed video. This impressed video is included in the features as an input\n    to the model.\n\n    Architecturally, the ranking model will be similar to the candidate generator\n    such that it will also take on a \"tower\" deep neural network. The final layer\n    of the network will use a sigmoid function to output the probability of the user\n    clicking on the candidate video. Each of the candidates will then be sorted\n    based on these click probabilities. A dozen or so videos at the top of this list\n    will be shown to the user as recommendations.\n\n    The ranker should be evaluated in terms of its recall.\n</p>\n<p>\n    As our model currently stands, it won't account for several biases. We need to\n    make sure that our model mitigates the following:\n    <ul>\n        <li>Model the freshness of videos</li>\n        <li>Discount popular videos</li>\n        <li>Limit the positive feedback loop created by the model</li>\n        <li>Prevent exploitation of the site structure</li>\n        <li>Prevent highly active users from overinfluencing the loss</li>\n        <li>Discouraging click-bait</li>\n    </ul>\n\n    The first bias we should consider is the fact that users favor videos that are\n    new (or fresh). We should add a feature to both the candidate generator as well\n    as the ranking model which represents the days since the video was posted. This\n    allows the model to build an awareness of how fresh a video is. We can even make\n    the feature value negative in the case that an uploader scheduled a post through\n    the UI. This feature will have to be normalized just as the other features are.\n\n    As well, our model will likely favor more popular videos since most of the features\n    are derived from video watches. If we don't account for some videos being more\n    popular than others, then our model could over exploit popular videos instead\n    of exploring more relevant videos that are less popular. We can mitigate this\n    bias by downsampling videos in the training examples in proportion to their\n    popularity.\n\n    In line with the trade-off of exploration vs. exploitation, we should be sure\n    to include video watches that weren't a direct result of our recommendation model.\n    We can accomplish this by providing training examples to the network that came\n    from views of videos embedded on other websites.\n\n    As well, the site structure can be learned and exploited by the model if search\n    queries are sequentially tied to video views, which we don't want. For instance,\n    if a user searches for something then watches the top video of the search results,\n    then the model would learn that a particular search history is associated strongly\n    with a particular video watch. This will result in the model recommending videos\n    which user has already searched for - which is not a meaningful recommendation.\n    Fortunately, our model already mitigates this by having no sequential relationships\n    between search histories and videos views due to separately averaging each of\n    their embeddings.\n\n    We also need to prevent highly active users from over influencing the loss\n    during training. This naturally happens because the training examples will\n    include far more instances from users who use the platform often. If we ensure\n    that each user has the same number of associated training examples in the\n    training set, then each user will be represented the same number of times which\n    will prevent the overinflunce of highly active users.\n\n    Lastly, the ranking model is trained on click through rate (which is whether\n    a user clicked on a recommended video impression). This can lead to biasing\n    the model toward click-bait. Click-bait is defined as a video which is good\n    at attracting clicks (by means of an attractive thumbnail or title) but which\n    doesn't retain the user once they begin watching the video. This often is the\n    result of the video not delivering on the promise impliplied by the title\n    or thumbnail. To reduce the ranking model's bias for favoring click-bait, we\n    can instead weight the loss in terms of how long that particular user watched\n    the video supplied in the input of the model (and therefore impressed on the\n    user). Videos which were impressed on the user but not clicked will receive\n    a unit weight of one to indicate zero watch time for that video. Framing the\n    loss in this way means that the longer a user watched a video, the more\n    influence that example will have on the model. This will correct for the\n    click-bait bias.\n</p>"
        },
        {
            "title": "Cold-start Mitigators",
            "content": "<p>\n    One shortcoming of the current model is its inability to deal with new\n    users or and newly posted videos. This is often referred to as the cold-start\n    problem. To support recommendations for new users, we can add features to both\n    the candidate generator and the ranking model which don't depend on previous\n    watches or search history. Typically what's used is user demographics such as\n    the user's age, device, gender identity, and nationality. We'll use those features\n    here which can be required during signup. For new videos, we can obtain watches\n    as implicit features and dislikes as explicit features by recommending the videos\n    to an uploader's subscribers. This allows the model to incorporate impression\n    and watch features in future training examples and for immediate inference as well.\n</p>"
        },
        {
            "title": "Distributed Training, Tuning, and Evaluation",
            "content": "<p>\n  We need to train, tune, and evaluate models with hundreds of billions of\n  examples. As well, the models will have billions of parameters. To ensure that\n  our system can properly handle models of this caliber, we'll need to implement:\n</p>\n<ul>\n    <li>Data Parallelism</li>\n    <li>Model Parallelism</li>\n    <li>Checkpointing</li>\n</ul>\n<p>\n    We will need to use a distributed training strategy which can also be leveraged\n    for model tuning.\n\n    We can use asynchronous stochastic gradient descent or a variant optimization\n    technique thereof. We'll use a parameter server architecture here. This means\n    a parameter server cluster will be responsible for maintaining billions of model\n    parameters. Worker nodes will be responsible for fetching parameters from the\n    parameter server as well as obtaining the location of a relevant subset of training\n    examples from an example queue. The example queue maintains a queue of mini-\n    batches to be trained on. To produce these random mini-batches, we need a randomly\n    shuffled view of the training set can and this can be handled by a dedicated\n    service which we'll refer to as the Training Manager.\n\n    The worker nodes will train the parameters fetched from the parameter server\n    on the training examples referenced by the polled message from the example queue.\n    This architecture can also be used for distributed model tuning and evaluation.\n\n    Since we selected an asynchronous distributed training architecture, we can\n    trade off parameter staleness with the bandwidth requirements between the\n    parameter server and worker nodes. For example, the workers can train on several\n    mini-batches retrieved from the example queue before communicating with the\n    parameter server to update the global parameter set.\n</p>\n<p>\n    As we mentioned earlier, the model parameters can't fit on a single machine\n    so we'll need each worker node to actually be a cluster of GPU-equipped machines\n    which partition the model across each of the machine's GPUs. This can include\n    techniques like pipelining model parallelism and model partitioning.\n</p>\n<p>\n    A machine in a worker cluster can become unresponsive at any time so it will\n    benefit us to have some mechanism of model checkpointing. This means that\n    gradient calculations will be stored outside of the clusters. This way, in the\n    case that a worker machine goes down before it can communicate the parameter\n    update with the parameter server, then a new machine can be spun up and the\n    cluster can pick up where it left off in terms of training.\n</p>"
        },
        {
            "title": "Model Hosting",
            "content": "<p>\n    YouTube has over 100MM daily active users each of which can refresh the recommendation\n    home page any number of times. This means we need an efficient to provide roughly\n    one trillion total recommendations per day. This assumes each user on average\n    will look at the recommendation home page consisting of about 10 videos. Each\n    request should be on the order of tens of milliseconds of latency. As well,\n    if a user refreshes the recommendation page, we should ensure that the same\n    videos are not recommended again. The final recommendations will be the result\n    of inferences from two models:\n</p>\n<ul>\n    <li>Candidate Generator</li>\n    <li>Ranker</li>\n</ul>\n<p>\n    For candidate generation, we'll take advantage of two strategies. The first\n    is that we can use the dot product of the user embedding and video embedding\n    obtained from the candidate generator network. The higher this dot product,\n    the more similarity there is between the video and the user. Effectively, we\n    want to find the k nearest neighboring videos given a user in terms of their\n    dot product. These k nearest neighbors will be the candidates used for ranking.\n\n    The problem is that finding the nearest neighbors this way, referred to as\n    maximum inner product search (MIPS) across one million videos can take\n    prohibitively long when serving candidates in real time to users. So, we'll\n    need a way to trade off the accuracy of the nearest neighbors search in favor\n    of a reduced latency. This is commonly referred to as approximate nearest\n    neighbor search. The method we can use here is a variation of locality\n    sensitive hashing called asymmetric locality sensitive hashing (ALSH). The main\n    idea of ALSH is to partition the embeddings by their similarities and then\n    when searching for nearest neighbors, look only at the partition that the input\n    example falls into. This prevents the need to search through all of the\n    embeddings and instead isolates the search to a partition which will contain\n    similar embeddings. There's often a reduction in accuracy compared to an\n    exhaustive search but that tradeoff is accepted because the speed up makes the\n    model usable for real time use.\n\n    Instead of implementing this from scratch we can use ScaNN which comes from\n    Google. ScaNN is trained by learning partitions for the existing embeddings.\n    To search for the nearest neighbors of a provided input, the embeddings in the\n    top N closest partitions are sent to be scored in terms of an estimated dot product.\n    Finally, the top of those candidates are then rescored by their exact dot products.\n    The top K of those candidates are selected and sent off to be ranked by the\n    ranking model. Here our K can be a few hundred video embeddings.\n\n    This approximate nearest neighbors search is tuned for a desired MAP@k. We\n    can do that by adjusting the hyperparameters ScaNN. One hyperparameter is the\n    number of partitions to create during training. As the number of partitions grow,\n    the precision improves at the expense of speed. Another influential hyperparameter\n    is the number of partitions to consider for a provided input. For instance, if\n    we only consider the single closest partition to the input embedding, then\n    we only need to evaluate a fraction of the total embeddings. However, since\n    these partitions are not exact representations of locality, it could be the case\n    that other adjacent partitions contain even closer embeddings (and therefore\n    more relevant candidate videos). Finally, the last hyperparameter is how many\n    embeddings we want to rescore based on their estimated distances from the\n    input embedding. If we chose to only rescore 10 embeddings, then the calculation\n    will be far faster than if we choose to rescore the approximate closest 100\n    embeddings.\n\n    After tuning, we should be able to achieve a precision of roughly 95% all while\n    supporting a few thousand queries per second on a moderately sized machine\n    with 16 cores and 32 GB of memory. We'd maintain several servers behind a load\n    balancer to manage almost 100MM requests per day.\n\n    We can ensure the same videos are not recommended when a user refreshes the\n    recommendation page by rejecting candidates based on explicit feedback such\n    as dislikes or implicit feedback like not clicking on a particular video\n    impression.\n</p>\n<p>\n    For ranking, we'll simply have several machines behind a load balancer which\n    will parallely rank video candidates received from the candidate generator. The\n    candidates will be sorted by their scores according to the ranker's inference\n    and the subset of the highest scoring candidates will be shown to the user in\n    the form of a recommendation on the IU. To speed up inferences, we can purchase\n    machines equipped with enough RAM to fit our entire model and enough cores\n    to run hundreds of inferences in parallel.\n</p>"
        },
        {
            "title": "Bonus #1",
            "content": "<p>\n    One benefit of using a two-stage modelling approach - one for candidate generation\n    and the other for ranking - is that we can use more than one candidate generator.\n\n    For instance, another way we can generate relevant candidates with high precision\n    is by using something called a co-visitation graph. In this graph, each node\n    represents a video. An edge between two nodes is number of times those videos\n    are viewed within the same user session over the past 24 hours. As well, the\n    edges are normalized by the product of total views of the connected video nodes.\n\n    To generate candidates, we can select a video node that a user has interacted\n    with either implicitly (watch time) or explicitly (liking or adding to playlist)\n    and finding the adjacent video nodes with the largest edge values. To avoid\n    recommending videos that are too similar to the original video, we can create\n    a spanning tree around the node which at least some number of edges away from\n    the original video node.\n\n    These candidates can be included alongside the other candidate generator's\n    candidates to be ranked by the ranking model.\n</p>"
        },
        {
            "title": "System Diagram",
            "content": "<img\n    width=\"100%\"\n    src=\"https://assets.algoexpert.io/course-assets/mlexpert/youtube_recommendation_system.svg\"\n    alt=\"Final Systems Architecture\"\n    style=\"background-color:white;\"\n/>"
        }
    ],
    "hints": [
        {
            "question": "Do we have to design YouTube's user interface?",
            "answer": "No, you can assume that the interface is already designed. Your system should simply provide videos to display as recommendations on the home page."
        },
        {
            "question": "Do we have to design a way to train the model that we'll use?",
            "answer": "Yes, you should detail how the model that you'll use will be trained."
        },
        {
            "question": "Do we have to design a way for scientists and ML engineers to do model development?",
            "answer": "No, that's out of the scope of this question."
        },
        {
            "question": "Do we need to design the way that our model will be hosted for real-time users?",
            "answer": "Yes, you'll need to cover how live users will receive recommendations from your model."
        },
        {
            "question": "Do we need to have our system perform online evaluations such as A/B tests?",
            "answer": "No, we're only concerned with designing a model\u2014not measuring the model's performance against an existing model."
        },
        {
            "question": "Do we need to design a data pipeline that transforms raw data into features and labels?",
            "answer": "No, that system will be provided for you."
        },
        {
            "question": "Do we have to support model retraining or online learning?",
            "answer": "No, you can assume that the model doesn't need retraining or online learning."
        },
        {
            "question": "Can we assume that we have access to a cloud-compute-infrastructure platform such as Google Cloud Platform?",
            "answer": "Yes, that's fine to use here."
        }
    ]
}