{
    "uid": "neuron",
    "testStrategy": "JSON",
    "name": "Neuron",
    "version": 0,
    "releaseDate": "2021-05-24T00:00:00Z",
    "category": "Model Concepts",
    "difficulty": 0,
    "acl": {
        "isFree": false,
        "isFreeForStudents": false,
        "productRequired": [
            "mlexpert"
        ],
        "isAvailable": true
    },
    "languagesSupported": [
        "python"
    ],
    "submissionStatistics": {
        "correctCount": 595,
        "failureCount": 122
    },
    "assessmentSummary": null,
    "video": {
        "vimeoId": "552661182",
        "duration": 0,
        "annotations": [],
        "instructor": "Ryan Doan",
        "overviewTime": 0,
        "codeWalkthroughTime": 366
    },
    "prompt": "<div class=\"html\">\n<p>Create a single neuron, to be used in a neural network.</p>\n<p>\n  You'll have access to a list <span>Neuron.examples</span> of 100 examples,\n  where each example is a dictionary with two keys: <span>\"features\"</span> and\n  <span>\"label\"</span>. The value of <span>\"features\"</span> is a list of 3\n  features, which have been min-max scaled for you, and the value of\n  <span>\"label\"</span> is <span>0</span> or <span>1</span>.\n</p>\n<p>Below is an example portion of the <span>Neuron.examples</span>:</p>\n<pre>\n[\n  {\"features\": [0.7737498370415932, 0.893981580520576, 0.7776116731845149], \"label\": 0},\n  {\"features\": [0.8356527294792708, 0.7535044575176968, 0.7940884252881397], \"label\": 0},\n  <span class=\"CodeEditor-promptComment\"># ...</span>\n  <span class=\"CodeEditor-promptComment\"># More examples.</span>\n  {\"features\": [0.25835793676162827, 0.2166447564607853, 0.5066866046843734], \"label\": 1},\n  {\"features\": [0.34848185391755987, 0.15010261370695727, 0.3466287718524547], \"label\": 1},\n  <span class=\"CodeEditor-promptComment\"># ...</span>\n  <span class=\"CodeEditor-promptComment\"># More examples.</span>\n]\n</pre>\n<p>Note that:</p>\n<ul>\n  <li>You shouldn't use regularization.</li>\n  <li>You should use mini-batch gradient descent.</li>\n  <li>You should use the sigmoid activation function.</li>\n  <li>\n    The neuron should include a weight for the bias and one weight for each\n    input feature.\n  </li>\n  <li>\n    You should use <span>0.01</span> for the learning rate, 10 examples for the\n    mini-batch size, 100 epochs, and the log loss as the loss function.\n  </li>\n  <li>\n    The <span>predict</span> function should return the probability that the\n    input should have the label of <span>1</span>\u2014not the corresponding\n    <span>0</span> or <span>1</span> label.\n  </li>\n  <li>\n    Your output value will automatically be rounded to the fourth decimal.\n  </li>\n</ul>\n<p></p>\n<h3>Sample Input</h3>\n<pre>\n<span class=\"CodeEditor-promptParameter\">features</span> = [0.79, 0.89, 0.777]\n</pre>\n<h3>Sample Output</h3>\n<pre>\n0.1636 <span class=\"CodeEditor-promptComment\">// The probability that the sample input should have a label of 1.</span>\n</pre>\n</div>",
    "hints": [
        "<p>\n  First, we need to extract mini-batches from the examples. This means\n  retrieving a portion of total examples, which will be used to complete one\n  iteration of gradient descent.\n</p>\n",
        "\n<p>\n  Now that we have a mini-batch of examples, we need to get the prediction for\n  each example in the mini-batch. This means performing a forward pass through\n  the neuron.\n</p>\n",
        "\n<p>\n  A forward pass for a single example involves multiplying each feature in the\n  example by each weight. Don't forget to include a bias in the features, to go\n  with the corresponding weight. The bias will just be a <span>1</span> appended\n  to the features, and the weight for the bias will be treated as just another\n  weight. Then, the results of these multiplications are summed up and finally\n  passed through a sigmoid to obtain the prediction.\n</p>\n",
        "\n<p>\n  Sigmoids take the equation: <span>1/(1 + e^-(x))</span>. In Python, you'll use\n  the <span>math.exp()</span> function for <span>e</span>.\n</p>\n",
        "\n<p>\n  After we have the prediction for each example in the mini-batch, we need to\n  find the gradient of the loss function with respect to each weight in the\n  neuron. The gradient with respect to any weight <span>i</span> is the\n  difference between the prediction and the true label of an example, multiplied\n  by <span>feature_i</span>. Since we're using a mini-batch, we sum each\n  gradient produced by each example. Finally, we divide each gradient by the\n  mini-batch size.\n</p>\n",
        "\n<p>\n  Now that we have 4 gradients (3 for the inputs and 1 for the bias), we need to\n  update the weights as follows:\n  <span>w_i(t+1) = w_i(t) - lr*gradient_i</span>, where <span>lr</span> is the\n  learning rate.\n</p>\n",
        "\n<p>\n  Now that the weights are updated, we continue through the remaining\n  mini-batches until a single epoch is completed; we then repeat the entire\n  process for 99 more epochs.\n</p>\n",
        "\n<p>\n  Now that the neuron is trained, we can get a prediction by performing a\n  forward pass with any input features.\n</p>"
    ],
    "spaceTime": "",
    "notes": "",
    "isSlowExecution": false,
    "isLongOutput": false,
    "visualization": {
        "inputType": null,
        "outputType": null
    },
    "resources": {
        "python": {
            "language": "python",
            "solutionsDisabled": false,
            "startingCode": "import numpy as np\n\n\nclass Neuron:\n    # Don't change anything in the `__init__` function.\n    def __init__(self, examples):\n        np.random.seed(42)\n        # Three weights: one for each feature and one more for the bias.\n        self.weights = np.random.normal(0, 1, 3 + 1)\n        self.examples = examples\n        self.train()\n\n    # Don't use regularization.\n    # Use mini-batch gradient descent.\n    # Use the sigmoid activation function.\n    # Use the defaults for the function arguments.\n    def train(self, learning_rate=0.01, batch_size=10, epochs=200):\n        # Write your code here.\n        pass\n\n    # Return the probability\u2014not the corresponding 0 or 1 label.\n    def predict(self, features):\n        # Write your code here.\n        pass\n",
            "solutions": [
                "# Copyright \u00a9 2023 AlgoExpert LLC. All rights reserved.\n\nimport numpy as np\nimport math\n\n\nclass Neuron:\n    # Don't change anything in the `__init__` function.\n    def __init__(self, examples):\n        np.random.seed(42)\n        # Three weights: one for each feature and one more for the bias.\n        self.weights = np.random.normal(0, 1, 3 + 1)\n        self.examples = examples\n        self.train()\n\n    # Don't use regularization.\n    # Use mini-batch gradient descent.\n    # Use the sigmoid activation function.\n    # Use the defaults for the function arguments.\n    def train(self, learning_rate=0.01, batch_size=10, epochs=200):\n        for _ in range(epochs):\n            for batch_window in range(len(self.examples) // batch_size):\n                mini_batch = self.examples[0 + (batch_size * batch_window) : batch_size + (batch_size * batch_window)]\n                predictions_labels = [\n                    {\"prediction\": self.predict(example[\"features\"]), \"label\": example[\"label\"]}\n                    for example in mini_batch\n                ]\n                gradients = self.__get_gradients(mini_batch, predictions_labels)\n                self.weights = self.weights - [learning_rate * gradient for gradient in gradients]\n\n    # Return the probability, not the corresponding 0 or 1 label.\n    def predict(self, features):\n        model_inputs = features + [1]\n        wTx = 0\n        for i, model_input in enumerate(model_inputs):\n            wTx = wTx + self.weights[i] * model_input\n        return 1 / (1 + math.exp(-wTx))\n\n    def __get_gradients(self, batch, predictions_labels):\n        errors = [\n            predictions_label[\"prediction\"] - predictions_label[\"label\"] for predictions_label in predictions_labels\n        ]\n        gradients = [0] * len(self.weights)\n        for example_i, example in enumerate(batch):\n            features = example[\"features\"] + [1]\n            for feature_i, feature in enumerate(features):\n                gradients[feature_i] = gradients[feature_i] + errors[example_i] * feature\n        gradients = [gradient / len(batch) for gradient in gradients]\n        return gradients\n"
            ],
            "sandboxCode": "# This file is initialized with a code version of some of\n# this question's test cases. Feel free to add, edit, or\n# remove test cases in this file as you see fit!\n\nimport program\nimport unittest\nimport pickle\nimport os\n\n_DATA_DIR = os.path.dirname(os.path.realpath(__file__)) + \"/data\"\n\n\nwith open(f\"{_DATA_DIR}/neuron.pickle\", \"rb\") as handle:\n    examples = pickle.load(handle)\nneuron = program.Neuron(examples)\n\n\nclass TestProgram(unittest.TestCase):\n    def test_case_1(self):\n        expected = 0.1636\n        actual = neuron.predict([0.79, 0.89, 0.777])\n        self.assertEqual(round(actual, 4), expected)\n\n    def test_case_2(self):\n        expected = 0.1551\n        actual = neuron.predict([0.8066217554804018, 0.863578574493143, 0.8610858626987106])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_3(self):\n        expected = 0.331\n        actual = neuron.predict([0.7, 0.6, 0.5])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_4(self):\n        expected = 0.7287\n        actual = neuron.predict([0.1, 0.2, 0.3])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_5(self):\n        expected = 0.64\n        actual = neuron.predict([0.2, 0.3, 0.4])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_6(self):\n        expected = 0.9684\n        actual = neuron.predict([-0.3, -0.4, -0.5])\n        self.assertEqual(round_to_4(actual), expected)\n\n\ndef round_to_4(number):\n    if not is_number(number):\n        # Bad output; let tests fail.\n        return number\n\n    return round(number, 4)\n\n\ndef is_number(element):\n    return type(element) in (int, float)\n",
            "unitTests": "import program\nimport unittest\nimport pickle\nimport os\n\n_DATA_DIR = os.path.dirname(os.path.realpath(__file__)) + \"/data\"\n\n\nwith open(f\"{_DATA_DIR}/neuron.pickle\", \"rb\") as handle:\n    examples = pickle.load(handle)\nneuron = program.Neuron(examples)\n\n\nclass TestProgram(unittest.TestCase):\n    def test_case_1(self):\n        expected = 0.1636\n        actual = neuron.predict([0.79, 0.89, 0.777])\n        self.assertEqual(round(actual, 4), expected)\n\n    def test_case_2(self):\n        expected = 0.1551\n        actual = neuron.predict([0.8066217554804018, 0.863578574493143, 0.8610858626987106])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_3(self):\n        expected = 0.331\n        actual = neuron.predict([0.7, 0.6, 0.5])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_4(self):\n        expected = 0.7287\n        actual = neuron.predict([0.1, 0.2, 0.3])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_5(self):\n        expected = 0.64\n        actual = neuron.predict([0.2, 0.3, 0.4])\n        self.assertEqual(round_to_4(actual), expected)\n\n    def test_case_6(self):\n        expected = 0.9684\n        actual = neuron.predict([-0.3, -0.4, -0.5])\n        self.assertEqual(round_to_4(actual), expected)\n\n\ndef round_to_4(number):\n    if not is_number(number):\n        # Bad output; let tests fail.\n        return number\n\n    return round(number, 4)\n\n\ndef is_number(element):\n    return type(element) in (int, float)\n"
        }
    },
    "customInputVars": [
        {
            "name": "features",
            "example": [
                0.79,
                0.89,
                0.777
            ],
            "schema": {
                "items": {
                    "type": "number"
                },
                "maxItems": 3,
                "minItems": 3,
                "type": "array"
            }
        }
    ],
    "tests": [
        {
            "features": [
                0.79,
                0.89,
                0.777
            ]
        },
        {
            "features": [
                0.8066217554804018,
                0.863578574493143,
                0.8610858626987106
            ]
        },
        {
            "features": [
                0.7,
                0.6,
                0.5
            ]
        },
        {
            "features": [
                0.1,
                0.2,
                0.3
            ]
        },
        {
            "features": [
                0.2,
                0.3,
                0.4
            ]
        },
        {
            "features": [
                -0.3,
                -0.4,
                -0.5
            ]
        }
    ],
    "jsonTests": [
        {
            "features": [
                0.79,
                0.89,
                0.777
            ]
        },
        {
            "features": [
                0.8066217554804018,
                0.863578574493143,
                0.8610858626987106
            ]
        },
        {
            "features": [
                0.7,
                0.6,
                0.5
            ]
        },
        {
            "features": [
                0.1,
                0.2,
                0.3
            ]
        },
        {
            "features": [
                0.2,
                0.3,
                0.4
            ]
        },
        {
            "features": [
                -0.3,
                -0.4,
                -0.5
            ]
        }
    ],
    "changelog": []
}